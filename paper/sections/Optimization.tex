% !TeX root = ../thesis.tex
\section{Performance explorations}
\subsection{Shared Memory}
Reducing use of register could open up for higher thread-configuration for some plans and may make kernels execute faster even if 32 or less registers were used (see section \ref{subsubsec:occupancy}).
As the fastest general purpose alternative to registers, shared memory is the only real alternative type of memory.
As shared memory requires array-allocation for the entire block of threads, it does require the kernel to know how many threads per block there will be during kernel compilation.
This is no big sacrifice as this was indirectly a requirement for the parameterized solutions in any case.
Another limitation is the total size of the shared memory which is 48KiB per SM.

The elements that could be moved to shared memory are the parameter arrays and the temporary arrays used by the Runge-Kutta 4 solver.
A working copy of the result array could also potentially be moved to shared memory, but it would still require the result array to be copied back to global memory at some point.
It is unlikely that this would lead to an increase in performance.
The $Va$ array is also only read from once and is unlikely to be a good candidate to copy to shared memory.

As a big source of local memory used was the temporary arrays $k1$, $k2$, $k3$, $k4$, $tmp$ and $v$ with the number of states in the plan as the size, these were moved to shared memory.
It is worth noting some properties of the kernels for the various plans before and after.
The number of registers can be seen in table \ref{table:tempArraysSharedMemory}.
It is interesting to see that the storing the temporary arrays in local memory actually uses the least registers for floats, considering that the temporary arrays should no longer be stored in them.
It is possible that it is using the added registers as a cache for accessing the shared memory.
It is also interesting that marking the memory as volatile - meaning that the compiler should not optimize reads and writes for the array - reduces the register usage slightly for floats.
It also reduces the number of registers a more significant number for doubles, almost to the point that 1024 threads per block can be used.

\begin{table}[h!]
\centering
{\setlength{\extrarowheight}{2pt}{\setlength{\tabcolsep}{3pt}
\begin{tabular}{ | l | r | r | r | r | r | r | }
  \hline
								&	Local32	&	Shared32	&	Shared32v	&	Local64	&	Shared64	&	Shared64v	\\ \hline
Pure Endowment					&	20		&	22			&	22			&	32		&	33			&	29			\\ \hline
Deferred Temporary Life Annuity	&	19		&	24			&	23			&	34		&	35			&	30			\\ \hline
Temporary Life Annuity Premium	&	19		&	24			&	23			&	34		&	35			&	30			\\ \hline
Term Insurance					&	20		&	24			&	24			&	34		&	35			&	30			\\ \hline
Disability Annuity				&	25		&	26			&	25			&	45		&	46			&	34			\\ \hline
Disability Term Insurance		&	25		&	27			&	26			&	46		&	46			&	34			\\ \hline
\end{tabular}}}
\caption{Register usage using various types of memory for the temporary arrays in the parameter-less Runge-Kutta 4 solver. 32=single precision floats, 64 = double precision float. v = volatile memory.\label{table:tempArraysSharedMemory}}
\end{table}

Another aspect is the range to which shared memory can be used.
With a hard limit of 48KiB of shared memory per SM, divided by these six temporary arrays with size of the number of states divided by four bytes for single precision floats and eight bytes for double precision floats, the following number of total states is the following:
\begin{itemize}
\item Single precision floats allow for 2048 states (for example 1024 threads with two states).
\item Double precision floats allow for 1024 states (for example 512 threads with two states).
\end{itemize}

This does limit the applicability of shared memory, especially as life insurance policies can use significantly more states (for example the GF820 plan, see section \ref{subsubsec:gf820parallelized}).
To get around this, either runtime configurations with a low number of threads per block will have to be used, or only a subset of the temporary arrays can be moved to shared memory.

The results of moving the temporary arrays to shared memory resulted in a 12.24\% increase in runtime for floats which more than doubles when also marking the arrays as volatile.
This is somewhat understandable as the single precision floats did not use a lot of registers to begin with and shared memory is generally slower than registers.
When marking the arrays as volatile, foregoing compiler optimization), the runtime increases by 24.74\% compared to the original solution.
For double precision floats the situation is slightly different as it used a lot of registers, and it achieved a negligible (0.45\%) decrease in runtime.
Marking the arrays as volatile turned this into a 7.72\% increase instead.
All in all not very impressive for the simple example life insurance plans.

If instead we turn to a more advanced life insurance plan such as the limited implementation of the GF820 which uses 50 states, the results look a bit different.
As there are nowhere near enough registers for 50 states, the remaining states need to be stored in local (global) memory which is slow.
For double precision floats it ends up using all 63 available registers per thread and spills 8568 bytes to local memory.
With 50 states, only up to 20 threads per block (less than a warp) worth of shared memory is available, regardless the results seem positive.
Where execution using local memory takes about 5.52 seconds, using volatile shared memory takes only 3.97 seconds making it 27.66\% faster.
Non-volatile memory is slightly slower at 4.30 seconds, but is still 21.53\% faster.
It is somewhat surprising that the volatile memory is faster, as the kernel claims to use more local memory and an identical amount of shared memory as the non-volatile.
Why this is so is yet unknown.

For all the shared memory results see appendix \ref{app:shared_memory}.

%It is very much worth noting that these six life insurance plans are very simple, none using more than 3 states (2 non-final states).
%Using a significantly more advanced life insurance plan, such as the (limited) GF820 (see section \ref{subsubsec:gf820parallelized}), may yield different results.


%Mention gf820 impact.
%48k memory

\subsubsection{Occupancy}\label{subsubsec:occupancy}
Occupancy is a term used to describe the ratio between the active and maximum active warps for a kernel.
It is a useful concept as in order to hide latency from context switches on the GPU the memory bus needs to be saturated by having enough transactions in flight.
To increase the number of transactions either the occupancy or the instruction level parallelism (see section \ref{subsubsec:ilp}) should be increased.

Things that potentially limit occupancy is register usage, shared memory usage and the number of threads per blocks.
To calculate thread occupancy NVIDIA has provided a spreadsheet that takes the following parameters
\begin{itemize}
\item Compute Capability - which determines the following options
\item Total shared memory in bytes available
\item The number of threads per block
\item The number of registers per thread
\item Shared memory used per block in bytes
\end{itemize}

The method of limiting registers in Alea.cuBase did not work and as the Runge-Kutta 4 solver results in many different numbers of registers based on the life insurance plan, there is no general way of ensuring register limits.
The solver also only has limited changeable aspects with regards to registers and shared memory is subject to the number of states a given insurance plan uses.
This results in the only reliably changeable aspect being the number of threads per block subject to registers and shared memory usage of the kernel.

Testing showed that lower thread configurations often had low occupancy (not surprising), and higher thread configuration typically reached 67\% which many find is enough to saturate the bandwidth (cite).

\subsubsection{Instruction Level Parallelism}\label{subsubsec:ilp}
Instruction level parallelism (ILP) is a term that covers the number of instructions relative to the number of dependent instructions when using parallel execution.
For example, consider the program seen in code sample \ref{ilpexample}.
The first two instructions are not dependent on each other and can be executed simultaneously.
If we assume that each operation can be completed in one unit of time, then these three instructions can be completed in two units of time for an ILP of 3/2.

\begin{lstlisting}[caption=ILP example program, label=ilpexample]
a = b + c
d = e + f
g = a * d
\end{lstlisting}

It is no trivial task to increase ILP generally nor for the Runge-Kutta 4 solver and no particular work has been done to increase ILP for any of the solutions.
Further work could ideally focus on increasing the ILP, as it can have a greater effect that increasing occupancy.
It is questionably that the bandwidth is not saturated at higher configurations, so assuming that the typical use case includes hundreds of thousands of iterations it may be for naught.

\subsection{Expression reduction}\label{subsec:exprReduction}
Alea.cuBase claims to generate highly optimized CUDA code so simple kernel optimization might be a futile effort.
Nonetheless an optimization method $optimize$ was written that performs fairly simple arithmetic reductions.

It is based on recursive pattern matching that performs the following operations:
\begin{itemize}
\item If working with a non-arithmetic expressions such as a $Let$ or $Lambda$-expressions, call $optimize$ on its components
\item If working with a binary arithmetic expression reduce it based on the following rules:
	\begin{itemize}
	\item If working with an expression where both components evaluate to numbers, simply evaluate and return the number. This applies to both addition, subtraction, multiplication and division.
	\item If working with addition where one of the components is zero, return the non-zero component.
	\item If working with subtraction where the first component is zero, return a negation expression of the second component.
	\item If working with subtraction where the second component is zero, return the first component.
	\item If working with multiplication where either component is one, return the other component.
	\item If working with multiplication where either component is zero, return zero.
	\item If working with division where the first component is zero, return zero.
	\item If working with division where the second component is one, return the first component.
	\end{itemize}
\end{itemize}
It does one additional reduction which removes empty ($() : unit$) elements of a $Sequence$ of Expressions.

Comparing an optimized version to a non-optimized version shows an overall runtime reduction of about 2.39\% for single precision floats and 1.46\% for double precision.
See appendix \ref{app:optimization_performance} for all the timing results.

The fact that these simple reductions reductions improve the runtime of reasonable efficient kernels makes the reduction seem worthwhile.
As life insurance plans in CalcSpec can both be more advanced and unoptimized, this becomes particularly important.

Further possible reductions could include removing unreferenced let-declarations as well as more numerical and arithmetic reductions.
There are also more possible numerical reductions, as the first rule only works for the end of an expression ($... 5 - 10$) and otherwise not ($... 5 - 10 + t$).

\subsection{Artificial Collective Spouse Pension}
The artificial collective spouse pension (``grundform 820'' or GF820 in Danish) is a life insurance policy in which upon the death of the ensured, the spouse if one such exists, receives a potentially deferred life annuity.

\subsubsection{Traditional method of computation}
GF820 is traditionally calculated as a 3-tier process with an outer, middle and inner model.

The \emph{outer model} can be seen as a 2-state (active-dead) Markov-model where the transition cost to the death state is the \emph{middle model}, solved from 120 minus the age of the ensured to 0.
The \emph{middle model} can be seen as another 2-state Markov-model, but rather than using Thiele's equation it calculates $-gp(tau) \cdot h(eta, tau) \cdot Inner(eta, k, t)$ where $gp$ is the marriage-probability of a $tau$-aged individual and $h$ is the probability that a $tau$-aged individual is married to an $eta$-aged individual given that the first individual is married.
This is solved from 120 to 1. The middle model can also be solved using numerical integration.
The \emph{inner model} is another 2-state Markov-model with benefit paid being $indicator(s >= k)$ (indicator returns one if true, otherwise zero) and transition intensity being the spouse death mortality intensity, solved from 120 minus $eta$ to 0.

It is a fairly complex method of computation as is also apparent in its runtime which takes up to two and a half minutes on the CPU.
While the traditional method of computation can be run on the GPU as-is, it is not particularly suited for it as there is a lot of code branching making threads finish at very different times making them less efficient to execute in parallel.

\subsubsection{Parallelized method of computation}\label{subsubsec:gf820parallelized}
An alternative approach was suggested by Klaus Grue using a single-tier approach.
The Markov-model contains the active state for the insured, 125 intermediary states signifying death of the insured when married to a spouse of an age difference from -62 to +62 and a final state signifying either death of the spouse or death of the ensured when unmarried, as depicted in figure \ref{fig:gf820}.

\begin{figure}[h!]\centering
\includegraphics[scale=0.5]{gf820.png}
\caption{Markov-model for the parallelized GF820 Artificial Collective Spouse Pension plan.\label{fig:gf820}}
\end{figure}

The transition probability to one of the intermediary states $\mu_{ai}$ is the mortality intensity for the ensured at time $t + age$ times the marriage probability that a $t + age$ year old is married to a $t + age + diff$ year old where $diff$ is the age difference of the spouse and the ensured as well as the number of the state.
The ensured to final state $\mu_{ad}$ probability is the mortality intensity of the ensured at time $t + age$ times the probability that the ensured is not married at time $t + age$ (1 minus the marriage probability at the time).
The intermediary to final state $\mu_{id}$ probability is the mortality intensity of the spouse at the time (assuming the sexuality of the ensured does not change) of age $t + age + diff$.

This does potentially lead to a lot of cases where the ensured will either be married to someone yet to be born or someone significantly older than the oldest person alive today, which is a problem for the Gompertz-Makeham mortality intensity methods.
To lessen the impact of this, the result of these functions are constrained to the range 0 to 2 by adding a constraint method during the AST conversion.

This model of calculation can already be expressed in CalcSpec as is, but is somewhat cumbersome to write as there is little variability in the model.
The final CalcSpec should resemble code sample \ref{gf820cs}.
Note: The $\\var.expr$ is CalcSpec syntax for lambdas.

\begin{lstlisting}[language=calcspec, caption=GF820 CalcSpec, label=gf820cs]
calculation = {
  name = "GF820 - Collective artificial spouse pension", 
  algorithm = { type="Runge Kutta 4", parameters={ stepsize = 0.01 }}, 
  equations = { 
    0 = { r_j = rate, b_j = 0, mu_jk = { 
        1 = mu_ai(-62), 2 = mu_ai(-61), 
        ...,
        124 = mu_ai(61), 125 = mu_ai(62), 
        126 = \t.(GM_f(t + age)) * (1 - gp(t + age))
      }, b_jk = {  } }, 
    1 = { r_j = rate, b_j = 1, mu_jk = { 50 = mu_id(-62) }, b_jk = {}}, 
    2 = { r_j = rate, b_j = 1, mu_jk = { 50 = mu_id(-61) }, b_jk = {}}, 
    ...,
    124 = { r_j = rate, b_j = 1, mu_jk = { 50 = mu_id(61) }, b_jk = {}}, 
    125 = { r_j = rate, b_j = 1, mu_jk = { 50 = mu_id(62) }, b_jk = {}}, 
    126 = {  } }, 
  range = { from = 85, to = 0 }, 
  boundaryvalues = { 0 = 0, 1 = 0 }, 
  expressions = { 
    interestrate = 0.05, 
    age = 35, 
    rate(t) = interestrate, 
    GM_f(t) = constrain(0.0005 + 10 ^ (5.728 - 10 + 0.038 * (age + t))), 
    GM_m(t) = constrain(0.0005 + 10 ^ (5.88 - 10 + 0.038 * (age + t))), 
    mu_ai = \diff.\t.GM_f(t+age) * (gp(t+age) * (h(t+age+diff)(t+age))), 
    mu_id = \diff.\t.GM_m(t + age + diff) 
  }
}
\end{lstlisting}

To optimize this process, functionality was added to transform the CalcSpec AST based on the various variables of the GF820 plan.
See code sample \ref{gf820simplifiedcs} for an example of this.

\begin{lstlisting}[language=calcspec, caption=GF820 CalcSpec, label=gf820simplifiedcs]
calculation = { 
  name = "GF820 - Collective artificial spouse pension", 
  algorithm = { type="Runge Kutta 4", parameters={ stepsize = 0.01 }}, 
  equations = { 
    GF820 = { age=age, rate=rate, mu_insured=GM_f, mu_spouse=GM_m }
  }, 
  range = { from = 85, to = 0 }, 
  boundaryvalues = { 0 = 0, 1 = 0 }, 
  expressions = { 
    interestrate = 0.05,
    age = 35,
    rate(t) = interestrate,
    GM_f(t) = constrain(0.0005 + (10 ^ (5.728 - 10 + 0.038 * (age + t)))),
    GM_m(t) = constrain(0.0005 + (10 ^ (5.88 - 10 + 0.038 * (age + t))))
  }
}
\end{lstlisting}

\subsubsection{Issues}
When attempting to compile the generated kernel with Alea.cuBase, an unexpected stack overflow exception was thrown during kernel compilation by Alea.cuBase.
By reducing the range of intermediary states from -62...+62 to -24...+24 for double precision and -36...+36 for single precision it was compilable again.

Various changes were made hoping to increase the maximum number of intermediary states but to no avail.
They consisted of the following:
\begin{itemize}
\item Reducing the depth of the generated AST. This included using a sum variable for the various state transitions.
\item Reducing the local memory used by moving the temporary arrays to shared memory.
\end{itemize}

At this point it is hard to say if the single-tier method actually works. 
Comparing the results of the limited-state solution to the original solution, as can be seen in figure \ref{fig:gf820comparisongraph}, we see what could be a similar but distorted development, but without further states no conclusion can be reached.
For the results of both the traditional solution and the parallelized Alea.cuBase solution, see appendix \ref{app:gf820results}.

\begin{figure}[h!]\centering
\includegraphics[scale=0.75]{gf820comparisongraph.png}
\caption{GF820 original vs limited-state single-tier results\label{fig:gf820comparisongraph}}
\end{figure}
