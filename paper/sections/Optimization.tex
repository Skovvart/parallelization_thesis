% !TeX root = ../thesis.tex
\section{Performance explorations}
\subsection{Shared Memory}\label{subsec:sharedmem}
Reducing use of registers could open up for higher threads per block-configuration for some plans and may make kernels execute faster even if 32 or less registers were used (see section \ref{subsubsec:occupancy}).
As the fastest alternative general purpose memory, shared memory is the only real available option.
As shared memory requires array-allocation for the entire block of threads, it does require the kernel to know how many threads per block there will be during kernel compilation.
This is no big sacrifice as this was indirectly a requirement for the parameterized solutions already.
Another limitation is the total size of the shared memory which is 48KiB per SM.

The elements that could be moved to shared memory are the parameter arrays and the temporary arrays used by the Runge-Kutta four solver.
A working copy of the result array could also potentially be moved to shared memory, but it would still require the result array to be copied back to global memory at some point.
It is unlikely that this would lead to an increase in performance.
The $Va$ array is also only read from once per thread and is unlikely to be a good candidate to copy to shared memory.

As a big source of local memory used was the temporary arrays $k1$, $k2$, $k3$, $k4$, $tmp$ and $v$ with the number of states in the plan as their size, these were moved to shared memory.
It is worth noting some properties of the kernels for the various plans before and after.
The number of registers used can be seen in table \ref{table:tempArraysSharedMemory}.
It is interesting to see that the storing the temporary arrays in local memory actually uses the least registers for single precision floats, considering that the temporary arrays should no longer be stored in them.
It is possible that it uses the added registers as a cache for accessing the shared memory.
It is also interesting that marking the memory as volatile - meaning that the compiler should not optimize reads and writes for the array - reduces the register usage slightly for single precision floats.
It also reduces the number of registers a more significant number for double precision floats, almost to the point that 1024 threads per block can be used.

\begin{table}[h!]
\centering
{\setlength{\extrarowheight}{2pt}{\setlength{\tabcolsep}{3pt}
\begin{tabular}{ | l | r | r | r | r | r | r | }
  \hline
								&	L32		&	S32			&	S32v		&	L64		&	S64			&	S64v	\\ \hline
Pure Endowment					&	20		&	22			&	22			&	32		&	33			&	29			\\ \hline
Deferred Temporary Life Annuity	&	19		&	24			&	23			&	34		&	35			&	30			\\ \hline
Temporary Life Annuity Premium	&	19		&	24			&	23			&	34		&	35			&	30			\\ \hline
Term Insurance					&	20		&	24			&	24			&	34		&	35			&	30			\\ \hline
Disability Annuity				&	25		&	26			&	25			&	45		&	46			&	34			\\ \hline
Disability Term Insurance		&	25		&	27			&	26			&	46		&	46			&	34			\\ \hline
\end{tabular}}}
\caption{Register usage using various types of memory for the temporary arrays in the parameter-less Runge-Kutta four solver.\label{table:tempArraysSharedMemory}}
\caption*{L = local memory, S = shared memory, 32 = single precision floats, 64 = double precision float, v = volatile memory.}
\end{table}

Another aspect is the range to which shared memory can be used.
With a hard limit of 48KiB of shared memory per SM, divided by these six temporary arrays with size of the number of states divided by four bytes for single precision floats and eight bytes for double precision floats, the following number of total states is the following:
\begin{itemize}
\item Single precision floats allow for 2048 states (for example 1024 threads with two states).
\item Double precision floats allow for 1024 states (for example 512 threads with two states).
\end{itemize}

This does limit the applicability of shared memory, especially as life insurance policies can use significantly more states, for example the GF810 collective spouse pension mentioned in section \ref{subsubsec:gf810parallelized}.
To get around this, either runtime configurations with a low number of threads per block will have to be used, or only a subset of the temporary arrays can be moved to shared memory.

The results of moving the temporary arrays to shared memory resulted in a 12.24\% increase in runtime for single precision which more than doubled when also marking the arrays as volatile.
This is somewhat understandable as the single precision floats did not use a lot of registers to begin with and shared memory is generally slower than registers.
When marking the arrays as volatile, the runtime increases by 24.74\% compared to the original solution.
For double precision floats the situation is slightly different as it used a lot of registers, and it achieved a negligible (0.45\%) decrease in runtime.
Marking the arrays as volatile turned this into a 7.72\% increase instead.
All in all not very impressive for the simple example life insurance plans.

If instead we turn to a more advanced life insurance plan such as the limited implementation of the GF810 collective spouse pension, which uses 50 states, the results look a bit different.
As there are nowhere near enough registers for 50 states, the remaining states need to be stored in local (global) memory which is slow.
For double precision floats it ends up using all 63 available registers per thread and spills 2696 bytes to local memory.
With 50 states, the temporary arrays use $50*6*8=2400$ bytes of memory only allowing up to 20 threads per block (less than a warp) worth of shared memory. 
Regardless, the results seem positive.
Where execution using only local memory takes about 5.52 seconds, using volatile shared memory takes only 3.97 seconds making it 27.66\% faster.
Non-volatile memory is slightly slower at 4.30 seconds, but still with an improvement of 21.53\% compared to only local memory.
It is somewhat surprising that the volatile memory is faster, as the reads and writes should no longer be optimized by the compiler.
The volatile version does use slightly less local memory, 1064 bytes to 1088 bytes, which may be the reason.
For all shared memory test results see appendix \ref{app:shared_memory}.

\subsubsection{Occupancy}\label{subsubsec:test:occupancy}
Occupancy (see section \ref{subsubsec:occupancy}) is a term used to describe how optimally the GPU is utilized with regards to warps, and is affected by the number of threads per block, shared memory per block and registers per thread.
The method of limiting registers in Alea.cuBase did not work and as the Runge-Kutta four solver results in many different numbers of registers based on the life insurance plan, there is no general way of ensuring register limits.
The solver also only has limited changeable aspects with regards to registers and shared memory is subject to the number of states a given insurance plan uses.
This results in the only reliably changeable aspect being the number of threads per block subject to registers and shared memory usage of the kernel.

Testing showed that lower thread configurations often had low occupancy (not surprising), and higher thread configuration typically reached 67\% which many find is enough to saturate the bandwidth (see section 10.4 of \cite{cudacoccupancy}).

\subsection{Expression reduction}\label{subsec:exprReduction}
Alea.cuBase claims to generate highly optimized CUDA code so simple kernel optimization might be a futile effort.
Nonetheless an optimization method $optimize$ was written which performs fairly simple arithmetic reductions.

It is based on recursive pattern matching that performs the following operations:
\begin{itemize}
\item If working with a non-arithmetic expressions such as a $Let$ or $Lambda$-expressions, call $optimize$ on its components
\item If working with a binary arithmetic expression, call $optimize$ on its components and reduce based on the following rules:
	\begin{itemize}
	\item If working with an expression where both components evaluate to numbers, simply evaluate and return the number. This applies to both addition, subtraction, multiplication and division.
	\item If working with addition where either components is zero, return the other component.
	\item If working with subtraction where the first component is zero, return a negation expression of the second component.
	\item If working with subtraction where the second component is zero, return the first component.
	\item If working with multiplication where either component is one, return the other component.
	\item If working with multiplication where either component is zero, return zero.
	\item If working with division where the first component is zero, return zero.
	\item If working with division where the second component is one, return the first component.
	\end{itemize}
\end{itemize}
It does one additional dead code elimination reduction which removes empty ($() : unit$) elements of a $Sequence$ of Expressions.

Comparing an optimized version to a non-optimized version shows an overall runtime reduction of about 2.39\% for single precision floats and 1.46\% for double precision.
See appendix \ref{app:optimization_performance} for all the timing results.

The fact that these simple reductions reductions improve the runtime of reasonable efficient kernels makes the reduction seem worthwhile, especially as many of the optimizations are not used in the example life insurance plans.
As life insurance plans in CalcSpec can both be more advanced and unoptimized, this becomes particularly relevant.

Further possible reductions could include removing unreferenced $let$ declarations as well as more numerical and arithmetic reductions.
There are also more possible numerical reductions, as the first rule only works for the end of an expression ($... 5 - 10$) and otherwise not ($... 5 - 10 + t$).

\subsection{Collective Spouse Pension}\label{sub:gf810}
The collective spouse pension (``grundform 810'' or GF810 in Danish) is a life insurance policy in which upon the death of the insured, the spouse, if one such exists, receives a life annuity.

\subsubsection{Traditional method of computation}
GF810 is traditionally calculated as a three-tier process with an outer, middle and inner model.

The \emph{outer model} can be seen as a two-state (active-dead) Markov-model where the transition cost to the death state is the \emph{middle model}, solved from 120 minus the age of the insured to 0.
The \emph{middle model} can be seen as another two-state Markov-model, but rather than using Thiele's equation it calculates \\\lstinline$-gp(tau) * h(eta, tau) * Inner(eta, k, t)$ where $gp$ is the marriage-probability of a $tau$-aged individual and $h$ is the probability that a $tau$-aged individual is married to an $eta$-aged individual given that the first individual is married.
This is solved from 120 to 1. The middle model can also be solved using numerical integration.
The \emph{inner model} is another two-state Markov-model with benefit paid being $indicator(s >= k)$ (indicator returns one if true, otherwise zero) and transition intensity being the death mortality intensity of the spouse , solved from 120 minus $eta$ to 0.

It is a fairly complex method of computation as is also apparent in its execution time which takes up to two and a half minutes on the CPU.
While the traditional method of computation can be run on the GPU as-is, it is not particularly suited for it due to the long time it takes to execute and as there is a lot of code branching making threads finish at very different times making them less efficient to execute in parallel.

\subsubsection{Parallelized method of computation}\label{subsubsec:gf810parallelized}
An alternative approach was suggested by Klaus Grue of Edlund using a single-tier approach.
The Markov-model contains the active state for the insured, 125 intermediary states signifying death of the insured when married to a spouse of an age difference from -62 to +62 and a final state signifying either death of the spouse or death of the insured when unmarried, as depicted in figure \ref{fig:gf810}.

\begin{figure}[h!]\centering
\includegraphics[scale=0.5]{gf810.png}
\caption{Markov-model for the parallelized GF810 Artificial Collective Spouse Pension plan.\label{fig:gf810}}
\end{figure}
\newcommand*{\diff}{\ensuremath{\mathit{diff}}}
The transition probability to one of the intermediary states $\mu_{ai}$ is the mortality intensity for the insured at time $t + age$ times the marriage probability that a $t + age$ year old is married to a $t + age + \diff$ year old with $\diff$ being the age difference between the spouse and the insured, as well as the number of the state.
The insured to final state $\mu_{ad}$ probability is the mortality intensity of the insured at time $t + age$ times the probability that the insured is not married at time $t + age$ (1 minus the marriage probability at the time).
The intermediary to final state $\mu_{id}$ probability is the mortality intensity of the spouse at the time (assuming the sexuality of the insured does not change) of age $t + age + \diff$.

This does potentially lead to a lot of cases where the insured will either be married to someone yet to be born or someone significantly older than the oldest person alive today, which can be a problem for the Gompertz-Makeham mortality intensity methods.
To lessen the impact of this, the result of these functions is constrained to be no higher than two by adding a constraint method.

This model of calculation can already be expressed in CalcSpec as is, but is somewhat cumbersome to write as there is little variability in the model.
The final CalcSpec should resemble code sample \ref{gf810cs}.
Note that the \lstinline$\var.expr$ is CalcSpec syntax for lambdas. 
It is currently also required to update the $from$ value in the $range$ section to the value of 120-age, as this is currently not possible to have variable values for this in the current solution.
%\clearpage
\begin{lstlisting}[language=calcspec, caption=GF810 full CalcSpec, label=gf810cs]
calculation = {
  name = "GF810 - Collective spouse pension", 
  algorithm = { type="Runge Kutta 4", parameters={ stepsize = 0.01 }}, 
  equations = { 
    0 = { r_j = rate, b_j = 0, mu_jk = { 
        1 = mu_ai(-62), 2 = mu_ai(-61),
        ...,
        124 = mu_ai(61), 125 = mu_ai(62), 
        126 = \t.(GM_f(t)) * (1 - gp(t + age))
      }, b_jk = {  } }, 
    1 = { r_j = rate, b_j = 1, mu_jk = { 126 = mu_id(-62) }, b_jk = {}}, 
    2 = { r_j = rate, b_j = 1, mu_jk = { 126 = mu_id(-61) }, b_jk = {}}, 
    ...,
    124 = { r_j = rate, b_j = 1, mu_jk = { 126 = mu_id(61) }, b_jk = {}}, 
    125 = { r_j = rate, b_j = 1, mu_jk = { 126 = mu_id(62) }, b_jk = {}}, 
    126 = {  } }, 
  range = { from = 85, to = 0 }, 
  boundaryvalues = { 0 = 0, 1 = 0 }, 
  expressions = { 
    interestrate = 0.05, 
    age = 35, 
    rate(t) = interestrate, 
    GM_f(t) = constrain(0.0005 + 10 ^ (5.728 - 10 + 0.038 * (age + t))), 
    GM_m(t) = constrain(0.0005 + 10 ^ (5.88 - 10 + 0.038 * (age + t))), 
    mu_ai = \diff.\t.GM_f(t) * gp(t+age) * (h(t+age+diff)(t+age)), 
    mu_id = \diff.\t.GM_m(t + diff) 
  }
}
\end{lstlisting}

To optimize this process, functionality was added to transform the CalcSpec AST based on the variability of the GF810 plan.
See code sample \ref{gf810simplifiedcs} for an example of this which is converted back to the code in code sample \ref{gf810cs}.
\clearpage
\begin{lstlisting}[language=calcspec, caption=GF810 simplified CalcSpec, label=gf810simplifiedcs]
calculation = { 
  name = "GF810 - Collective spouse pension", 
  algorithm = { type="Runge Kutta 4", parameters={ stepsize = 0.01 }}, 
  equations = { 
    GF810 = { age=age, rate=rate, mu_insured=GM_f, mu_spouse=GM_m }
  }, 
  range = { from = 85, to = 0 }, 
  boundaryvalues = { 0 = 0, 1 = 0 }, 
  expressions = { 
    interestrate = 0.05,
    age = 35,
    rate(t) = interestrate,
    GM_f(t) = constrain(0.0005 + (10 ^ (5.728 - 10 + 0.038 * (age + t)))),
    GM_m(t) = constrain(0.0005 + (10 ^ (5.88 - 10 + 0.038 * (age + t))))
  }
}
\end{lstlisting}

\subsubsection{Issues}
When attempting to compile the generated kernel with Alea.cuBase, an unexpected stack overflow exception was thrown during kernel compilation by Alea.cuBase.
By reducing the range of intermediary states from -62...+62 to -24...+24 for double precision and -36...+36 for single precision it was compilable again.

Various changes were made hoping to increase the maximum number of intermediary states but to no avail.
They consisted of the following:
\begin{itemize}
\item Reducing the depth of the generated AST. This included using a sum variable for the various state transitions as well as expression reduction by the $optimize$ method described in section \ref{subsec:exprReduction}.
\item Reducing the local memory used by moving the temporary arrays to shared memory.
\end{itemize}

\subsubsection{Results}
As the Alea.cuBase solution did not support the required number of states, a CPU-version was implemented to test the validity of the new method.
For age 35 it provided identical results down to the 7th decimal and as a bonus it completed in 1060 ms on average - an improvement of close to factor 150 compared to the original implementation.
Another interesting fact is that for age 35, the limited 50-states Alea.cuBase solution with intermediary states from -24...+24 was also identical down to the 7th decimal, and to the 10th decimal compared to the -62...+62 CPU version.
The sum of the reserve at every year also only leaves a relative difference of about 0.02\%, while the final result (year zero) only has an difference in results of $1.08794 \cdot 10^{-8}$ which is a relative error of no more than factor $9.733 \cdot 10^{-9}$.
The relative difference of the sum of the reserves for the CPU version with all intermediary states staying under 0.09\% for ages tested up to 80, with the final result difference not exceeding $1.2535 \cdot 10^{-7}$ which is a relative error not exceeding factor $6.107 \cdot 10^{-8}$.

This is sadly not a feat the limited 50-states implementation can claim to achieve. 
With age 15 there is a 9.45\% relative difference in the final result while at age 50 there is a 68.34\% relative difference.
For a visualization of this, see figure \ref{fig:gf810ages}. 
Note that many of the entries can not be seen as they are overlapping due to being practically identical.
The cause of the results getting progressively worse as the age increases is due to the fact that the marriage-probability in the early years is incredibly small.
It is also because as the insured grows older, the younger spouses account for a lot more of the reserve than older spouses that die earlier.
Since there are not enough intermediary states to reach these young spouses, the results are skewed.

\begin{figure}[h!]\centering
\includegraphics[scale=0.3]{GF810-ages.jpg}
\caption{Comparison of GF810 reserve-output for ages 1, 15, 35, 50 and 80 using results from the original solution, a CPU-version of the new method and the limited state GPU-version of the new method.\label{fig:gf810ages}}
\end{figure}

To conclude, the method definitely works in theory even if not currently on the GPU.
It does require all the intermediary states unless certain ``sweet spots'' are hit, which can not be relied upon.

Speed-wise, it is hard to compare running-times without being able to utilize all the required states.
For the limited 50-states version with intermediary states from -24...+24, tests were performed in section \ref{subsec:sharedmem} which resulted in runtimes of 5.52 seconds using global memory, 3.97 seconds using volatile shared memory and 4.30 second using non-volatile shared memory, but especially the shared memory results are inaccurate for the full solution as there is simply not enough shared memory available for all states.
If not using shared memory (or at least no more than 96 bytes per thread), 512 threads per block would still be an option.
This would result in the remaining registers spilling to local memory which would decrease performance.
An optimist could claim that similar relative increases as seen in simpler plans could be achieved, which would still be a massive performance increase.
A pessimist could argue that the use of slow local memory and overall complexity would limit the performance significantly.
No matter which camp you might be in, it is very likely that given enough iterations there should still be a significant increase in performance when calculating a large enough number of iterations.

For double precision comparison results, see appendix \ref{app:gf810ageresults}.

\subsubsection{Preliminary GPU performance exploration}
As the GF810 model uses many states and as a consequence consumes a lot of memory, it begs to question what limits this imposes.
When only using local memory, up to 512 threads per block remains an option.
This however requires a significant amount of memory and may not be efficient.

The current limited GPU version uses 50 states and not the ideal 126.
For age 35, where the calculation range of the Runge-Kutta four solver (see section \ref{subsec:background:thielerungekutta}) is from year 85 (120-age) to 0, with double precision floats it has a memory usage of 63 registers and 2696 bytes of local memory per thread.
Increasing the number of states by one should in theory only affect the temporary arrays of the Runge-Kutta four solver, leading to an increase in local memory usage by 40 bytes for double precision floats (one more state in six temporary arrays times eight bytes for double precision floats).
This would result in the total amount of memory used for age 35 with double precision $(126 - 50) * 40 + 2696 = 5736$ bytes per thread, or at most $5736 * 512 * 14 = 41115648$ bytes (39.21 MiB) concurrent memory usage with 512 threads and all SMs running a block.
Testing showed that increasing the number of states by one increased the memory usage by more than 40, ranging from 40 to 57.5 bytes per state, but this might increase further for higher numbers of states.
Assuming a worst case of 58 bytes per state, the total memory consumption per thread would reach 7104 and concurrent memory usage would peak at about 48.56 MiB.
Other sources of memory usage include the result array stored in global memory which for age 35 would use up to $86 * 126 * 512 * 8 = 44384256$ bytes (42.33 MiB) per block with 512 threads per block.
Assuming a practical minimum of 14 blocks, this leads to a total concurrent GPU memory usage of 613.8MiB, a significant amount less than the 4GiB available global memory.

Working with large amounts of memory can also lead to potential problems for the CPU.
While compilation to x64 should allow for array sizes of up to 2,147,483,647 as arrays use an integer for indexing, and while the result array is still using significantly less memory than the 2GiB x64 single object restriction of the .NET Common Language Runtime, out of memory-errors were encountered on the test machine when allocating device memory and gathering the results from the GPU, potentially because of the relatively small amount of CPU RAM (4GiB).
This limited the range of the following performance tests.

The high memory throughput of the GF810 plan potentially gives it other properties than the simpler life insurance plans.
To discover what launch configuration are most efficient at high memory throughput is not so clear, but tests were performed to provide some insight despite only being done with a limited number of states.
See table \ref{table:gf810gpuperformance} for the main results of the test, and see appendix \ref{app:gf810performance} for all the test results.
Do note that it uses iterations per second rather than per millisecond as seen in previous tables due to the increased computational complexity.
The hypothesis that it is inefficient to use less than 14 blocks or 32 threads per block is reconfirmed even at high memory throughput.
See the rows of the table with launch parameters (7,512), (224, 16), (1, 32) and (1, 1) for examples of this.
The majority of the remaining launch parameters used no more than 3584 iterations (for example the launch parameters with 112 blocks and 32 threads per block) due to previously mentioned memory problems, but with varying levels of performance.
It seems that due to the high memory throughput it is preferable to increase the number of blocks over threads as long as at least 32 threads per block (a warp) is used.
Knowing that it seems most efficient at just one warp, means that multiple launches of the kernel is a possibility that could limit the size of the result array.
That does mean that functionality to skew the scope of the launch would have to be added.
Alternatively, as long as there is total faith in the results, only the final result of the first state in year 0 is typically required, eliminating memory issues when gathering the results from the GPU.

\begin{table}[h!]
\centering
{\setlength{\extrarowheight}{2pt}{\setlength{\tabcolsep}{3pt}
\begin{tabular}{ | l | r | r | }
  \hline
Launch parameters	&	Kernel time	&	Iterations per second	\\ \hline
(1, 1)				&	4.95		&	0.2						\\ \hline
(1, 32)				&	4.84		&	6.61					\\ \hline
(7, 512)			&	11.80		&	303.93					\\ \hline
(14, 32)			&	6.74		&	66.50					\\ \hline
(14, 128)			&	8.08		&	221.87					\\ \hline
(14, 256)			&	11.68		&	306.76					\\ \hline
(28, 128)			&	10.68		&	335.69					\\ \hline
(56, 64)			&	9.51		&	376.69					\\ \hline
(112, 32)			&	9.43		&	379.90					\\ \hline
(224, 16)			&	20.49		&	174.89					\\ \hline
\end{tabular}}}
\caption{Limited GF810 with age 35 GPU performance explorations with various launch parameters.\label{table:gf810gpuperformance}}
\caption*{Launch parameters are of the form (blocks, threads per block). Kernel time is in seconds.}
\end{table}

\subsubsection{Step-size experimentation}
A straight-forward way to increase performance is to reduce the number of steps used by the Runge-Kutta solver at the cost of increased error.
This was tested on the CPU-implementation of the new GF810 method with 16, 32 and 64 as the number of steps compared to the original 100 steps, and tested for ages 10 to 100 in 5 year intervals.
Similar relative performance impacts should be seen on the GPU.

Results showed that with just 16 steps an average relative error compared to 100 steps was no more than $6.18318 * 10^{-11}$ with an increase in performance of factor 6.24.
When using 32 steps, the error was reduced to $6.3476 * 10^{-12}$ with an increase in performance of factor 3.13, and with 64 as the number of steps, the error was reduced to $1.626 * 10^{-12}$ with an increase in performance by factor 1.57.

One important trend to note is the fact that as age increases, and as a result the years calculated decreases, the rate of error increases.
As the high-age calculations already require less computation it would be ideal to use a larger number of steps for these computations.

For the results of the various step-sizes, see appendix \ref{app:gf810stepsizes}.

