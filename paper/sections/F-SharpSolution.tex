% !TeX root = ../thesis.tex
\subsection{F\# Alea.cuBase Solution}
The F\# Alea.cuBase solution has many similarities to the CUDA C implementation.
One key factor is that the Runge-Kutta 4 kernel is implemented using Code Quotations, as are the plan-specific $dV$ and $bj\_ii$ methods.
As the plan-specific $dV$ and $bj\_ii$ typically include references to helper-methods, you could define them in terms of Quotations and use the splicing operators to mix them together.
Another option is to use use the \textit{[\textless{}ReflectedDefinition\textgreater{}]} F\# attribute on method calls which makes most regular methods accessible in Quotations.
An example of this can be seen in code sample \ref{cubase_pureendowment}. 
The various plans are in this case not implemented using classes, so each method name is prepended with a plan acronym ($pe\_$).
It also refers to come common constants and methods not defined in the class much like the previous implementations.

\begin{lstlisting}[language=FSharp, caption=The pure endowment insurance plan expressed in F\# Alea.cuBase, label=cubase_pureendowment]
[<ReflectedDefinition>] 
let pe_b_0 t = zero
[<ReflectedDefinition>]
let pe_mu_01 t = GM t
[<ReflectedDefinition>]
let pe_bj_00 t = if t = pensiontime then bpension else zero
[<ReflectedDefinition>]
let pe_bj_01 t = zero
let pe_dV = 
	<@ fun t (V:deviceptr<floatP>) (res:deviceptr<floatP>) -> 
		res.[0] <- r t * V.[0] - pe_b_0 t - pe_mu_01 t * (zero - V.[0] + pe_bj_01 t) @>
let pe_bj_ii = 
	<@ fun t (res:deviceptr<floatP>) ->
		res.[0] <- pe_bj_00 t @>
\end{lstlisting}

To run the plan there is one Runge-Kutta 4 kernel defined as can partially be seen in code sample \ref{cubase_rk4_n_snippet}. 
For the full implementation see appendix \ref{app:cubase_rk4_n}. 
What may be of interest is the $deviceptr$ type used by $Va$ and $result$, which function very similarly to pointers in C.
The temporary arrays may also be of interest. They are initiated using Alea.cuBase and then transformed into the generic $deviceptr$ type representations that work similarly to C pointers.

\begin{lstlisting}[language=FSharp, caption=The Runge-Kutta 4 solver expressed in F\# Alea.cuBase, label=cubase_rk4_n_snippet]
let RK4_n dV bj_ii states = cuda {
	let! kernel =
		<@ fun a b steps (Va:deviceptr<floatP>) (result:deviceptr<floatP>) ->
			//Calculate unique result offset
			let offset = (blockIdx.x * blockDim.x + threadIdx.x) * states * (a + 1)
            
			//Splice in other quotation expressions
			let dV = %dV
			let bj_ii = %bj_ii
			let limConv = %limConv //different limits for float and double

			let h   = -one / conv steps
			//Initialize reusable intermediary arrays
			let k1	  = __local__.Array<floatP>(states) |> __array_to_ptr
			let k2	  = __local__.Array<floatP>(states) |> __array_to_ptr
			let k3	  = __local__.Array<floatP>(states) |> __array_to_ptr
			let k4	  = __local__.Array<floatP>(states) |> __array_to_ptr
			let tmp	  = __local__.Array<floatP>(states) |> __array_to_ptr
			let v	  = __local__.Array<floatP>(states) |> __array_to_ptr
            
            ...Actual Runge-Kutta 4 implementation
        @> |> Compiler.DefineKernel 

    return Entry(fun program ->
        let worker = program.Worker
        let kernel = program.Apply kernel
        fun a b steps ->
            //Calculate size of result array
            let n = (a + 1) * states * blocks * threads
            //Allocate memory on device
            use Va = worker.Malloc(Array.zeroCreate<floatP> states)
            use result = worker.Malloc(Array.zeroCreate<floatP> n)

            let lp = LaunchParam (blocks, threads)
            ...Timing mechanism start
            //Launch kernel with parameters
            kernel.Launch lp a b steps Va.Ptr result.Ptr
            ...Timing mechanism end, save kernel execution time in ms1
            //Gather and return device results and kernel execution time
            let result = result.Gather()
            result, ms
        )
}
\end{lstlisting}

As the GPU code is generated at runtime, a lot of the compile-time-limitations of CUDA C disappear.
While the kernels may not take all types of parameters at launch-time, during kernel-compilation they can often be used.
The code is not making much use of the functional paradigm and memory-allocation for the device is still required, but lots of small changes like the $use$ keyword still make it faster and safer to program in F\# as opposed to CUDA C.

The solution also makes use of a method to compile the kernels for reuse and another to execute them, as can be seen in code sample \ref{cubase_compileandrun}. 

\begin{lstlisting}[language=FSharp, caption=Kernel compilation and execution methods in F\# Alea.cuBase, label=cubase_compileandrun]
let compile dV bj_ii states = RK4_n dV bj_ii states |> Compiler.load Worker.Default

let runKernel (program:Program<int->int->int->floatP[]*float>) a b = program.Run a b steps
\end{lstlisting}

The results of running the F\# Alea.cuBase solution shows that the highest number of calculations per ms were 98.33 for float precision and 25.30 ms for double precision.
For the full float results table \ref{table:cubaseManualfloattime} and for double results see table \ref{table:cubaseManualdoubletime}.
This is a slight improvement for the float version while being nearly identical for the double version.
Unlike the CUDA C version, I was not able to limit the maximum amount of registers used, making  it impossible to test configurations with 1024 threads.

For more information on result comparison see section \ref{subsec:result_comparison}.

\begin{table}[h!]
\centering
{\setlength{\extrarowheight}{2pt}{\setlength{\tabcolsep}{3pt}
\begin{tabular}{ | r | r | r | r | r | r | r | r | }
  \hline
\diaghead{Threads/Blocks}{Threads}{Blocks} &
1       & 	14 	 &	14*5 	& 	14*10 	& 14*20 & 14*25 & 14*30 \\ \hline
1       & 	0.02 &	0.34 	&	1.61	&	1.67	&	2.15	&	2.04	&	2.36	\\ \hline
8       & 	0.19 &	2.72 	&	12.87	&	13.33	&	17.13	&	16.28	&	18.80	\\ \hline
16     &	0.38 &	5.43	&	25.60	&	26.53	&	34.05	&	32.35	&	37.47	\\ \hline
32     &	0.76 &	10.78	&	50.58	&	52.59	&	67.61	&	64.27	&	74.51	\\ \hline
64     &	1.37 &	21.12	&	67.15	&	70.75	&	88.71	&	85.95	&	95.53	\\ \hline
128   &	1.69 &	37.36	&	77.74	&	84.81	&	85.36	&	88.89	&	92.55	\\ \hline
256   &	1.73 &	46.54	&	95.53	&	97.41	&	97.92	&	94.80	&	95.49	\\ \hline
512   &	6.75 &	48.20	&	97.44	&	98.01	&	98.23	&	96.28	&	98.31	\\ \hline
1024 &	6.93 &	96.11	&	97.94	&	98.16	&	98.31	&	98.33	&	98.29	\\ \hline
\end{tabular}}}
\caption{F\# Alea.cuBase calculations per ms with float precision\label{table:cubaseManualfloattime}}
\end{table}

\begin{table}[h!]
\centering
{\setlength{\extrarowheight}{2pt}{\setlength{\tabcolsep}{3pt}
\begin{tabular}{ | r | r | r | r | r | r | r | r | }
  \hline
\diaghead{Threads/Blocks}{Threads}{Blocks} &
			1     &	14	      &	14*5	&	14*10	&	14*20	&	14*25	&	14*30	\\ \hline
1     &	0.01	&	0.11	&	0.49	&	0.49	&	0.63	&	0.59	&	0.69	\\ \hline
8     &	0.06	&	0.87	&	3.93	&	3.91	&	5.01	&	4.74	&	5.51	\\ \hline
16   &	0.12	&	1.73	&	7.83		&	7.88		&	10.05	&	9.48	&	11.01	\\ \hline
32   &	0.25	&	3.46	&	15.63	&	15.75	&	20.06	&	18.94	&	22.02	\\ \hline
64   &	0.49	&	6.73	&	19.96	&	21.67	&	24.42	&	23.22	&	25.22	\\ \hline
128 &	0.96	&	11.26	&	20.31	&	25.18	&	24.10	&	24.94	&	25.34	\\ \hline
256 &	1.61	&	12.53	&	24.84	&	25.12	&	25.27	&	25.35	&	25.42	\\ \hline
512 &	1.79	&	25.03	&	25.26	&	25.28	&	25.30	&	25.30	&	25.25	\\ \hline
\end{tabular}}}
\caption{F\# Alea.cuBase calculations per ms with double precision\label{table:cubaseManualdoubletime}}
\end{table}

\subsection{Insurance plan parameters}
The parallelized versions have so far been running the same static plans over and over again which is not very useful in practical scenarios.
To make parallellisation useful, the ability to make constants parameterized is required.
These constants could for example be the interest rate or the age of the insured.

To achieve this, the $dV$ and $bj\_ii$ signatures were changed to include an additional array containing parameters for the insurance plan.
The parameters were shaped so each thread would have its own unique set of parameters laid in sequence. 
See figure \ref{} for an example of this.
The plans were then made responsible for knowing how many variables there were and which variables where which. 
They also had to pass the parameters to where they were needed, for example age to the mortality functions.
This iteration of the code also switched to using classes eliminating the need for prefixes for the different plan methods.
For an example of this, see code sample \ref{cubase_pureendowmentparams}.
The RK4\_n kernel was also altered to provide the thread-specific pointer to the plan parameter array and pass this to the updated $dV$ and $bj\_ii$ methods.
This position would be the position of the thread times the number of plan parameters.
For a brief overview of the changes to the RK4\_n kernel, see code sample \ref{cubase_rk4nparams}.

\begin{lstlisting}[language=FSharp, caption=Parameterized pure endowment life insurance plan in F\# Alea.cuBase, label=cubase_pureendowmentparams]
type PureEndowment (planParams, paramCount) =
    inherit Plan<floatP>(40, 0, 1, planParams, paramCount) with
        [<ReflectedDefinition>]
        let b_0 t = conv 0
    
        [<ReflectedDefinition>]
        let mu_01 t age = GM t age
    
        [<ReflectedDefinition>]
        let bj_00 t pensiontime bpension = if t = pensiontime then bpension else conv 0

        [<ReflectedDefinition>]
        let bj_01 t = conv 0

        override this.dV = <@ fun t (V:deviceptr<'T>) (planParams:deviceptr<'T>) (res:deviceptr<'T>) -> 
                let age = planParams.[0]
                res.[0] <- (r t) * V.[0] - (b_0 t) - (mu_01 t age) * (conv 0 - V.[0] + (bj_01 t))
            @>
        override this.bj_ii = <@ fun t (planParams:deviceptr<'T>) (res:deviceptr<'T>) -> 
		    let bpension = planParams.[1]                
                 let pensiontime = planParams.[2]
                res.[0] <- bj_00 t pensiontime bpension
            @>
\end{lstlisting}
%insert visualisation of the parameter array

Results were nearly identical for floats with the highest being xx.xx calculations per ms, but almost twice as fast for doubles with xx.xx calculations per ms. 
For the full results see tables \ref{table:cubaseManualParamsfloattime} and \ref{table:cubaseManualParamsdoubletime} below.
The increase in the double results must be because of \emph{...speculate...}.

This shows that parameters not only barely affect the running time and are therefore more than viable, it also shows that the double results can be optimized a good bit which is hopeful.

\begin{table}[h!]
\centering
{\setlength{\extrarowheight}{2pt}{\setlength{\tabcolsep}{3pt}
\begin{tabular}{ | r | r | r | r | r | r | r | r | }
  \hline
\diaghead{Threads/Blocks}{Threads}{Blocks} &
\end{tabular}}}
\caption{F\# Alea.cuBase calculations per ms with float precision and parameters\label{table:cubaseManualParamsfloattime}}
\end{table}

\begin{table}[h!]
\centering
{\setlength{\extrarowheight}{2pt}{\setlength{\tabcolsep}{3pt}
\begin{tabular}{ | r | r | r | r | r | r | r | r | }
  \hline
\diaghead{Threads/Blocks}{Threads}{Blocks} &
\end{tabular}}}
\caption{F\# Alea.cuBase calculations per ms with double precision and parameters\label{table:cubaseManualParamsdoubletime}}
\end{table}

\begin{lstlisting}[language=FSharp, caption=Parameterized RK4\_n kernel in F\# Alea.cuBase, label=cubase_rk4nparams]
let inline RK4_n (plan:Plan<_>) = cuda {
    let states = plan.states
    let! kernel =
        <@ fun (a:int) b steps (Va:deviceptr<floatP>) (d_params:deviceptr<floatP>) (result:deviceptr<floatP>) ->
            let offset = (blockIdx.x * blockDim.x + threadIdx.x) * states * (a + 1)
            if blockIdx.x * blockDim.x + threadIdx.x > plan.planParams.Length/plan.paramCount then () else

            let localParams = d_params.Ptr ((threadIdx.x + blockIdx.x*blockDim.x)*plan.paramCount)
            let dV = %plan.dV
            let bj_ii = %plan.bj_ii
            ...
            while y > b do
                bj_ii (conv y) localParams v
                saxpy (conv 1) v (result.Ptr (offset + y * states)) v states
                for s = 0 to steps - 1 do // Integrate backwards over [y, y-1]
                    ....
                    dV (if s = 0 then t - limit else t) v localParams k1
                    ...
                    dV (t + h / conv 2) tmp localParams k2
                    ...
                    dV (t + h / conv 2) tmp localParams k3
                    ...
                    dV (if s = steps - 1 then t + h + limit else t + h) tmp localParams k4
                    ...
                arrayCopy v (result.Ptr (offset + y*states - states)) states
                y <- y - 1
        @> |> Compiler.DefineKernel 

    return Entry(fun program ->
        ...
        fun a b steps blocks threadsPerBlock ->
            ...
            use planParams = worker.Malloc<'T>(plan.planParams)
            ...
            let msec, _ = time false "" (fun () -> kernel.Launch lp a b steps Va.Ptr planParams.Ptr result.Ptr)
            ...
        )
}
\end{lstlisting}

Talk about parameter generation and division of work (or wait until calcspec? dunno)


todo:?
RK4 step image
delta numerical critique
visual ast of quotation?